{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a377868c-a0c3-4135-af94-41172a47d164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/50, Loss: 1.5828711890170256e+20\n",
      "Epoch: 20/50, Loss: 1.2076298606896323e+20\n",
      "Epoch: 30/50, Loss: 9.99670342474405e+19\n",
      "Epoch: 40/50, Loss: 8.834692917217355e+19\n",
      "Epoch: 50/50, Loss: 8.091825038291894e+19\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def train_test_split(df, test_size=0.3):\n",
    "    test_index = int(len(df) * test_size)\n",
    "    train_data = df.iloc[:-test_index]\n",
    "    test_data = df.iloc[-test_index:]\n",
    "    return train_data, test_data\n",
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, encoding_dim)\n",
    "        self.decoder = nn.Linear(encoding_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Train and save the autoencoder model\n",
    "def train_and_save_model(train_data, encoding_dim, num_epochs, batch_size):\n",
    "    input_dim = train_data.shape[1]\n",
    "    model = Autoencoder(input_dim, encoding_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    train_data = torch.from_numpy(train_data.values).float()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, train_data.size(0), batch_size):\n",
    "            inputs = train_data[i:i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'autoencoder_model_1.pth')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load your dataset\n",
    "    df = pd.read_csv('AI_sample1.csv')\n",
    "    df=df[['BASE_CCY_AMT', 'FNCT_CCY_AMT', 'TXN_CCY_AMT', 'IB_FLAG', 'TXN_CCY', 'OPER_DPST_FLAG', 'LV_DMC_CTRY', 'FRS_ACCOUNT_CLASS','ACCT_TYP', 'ACTG_UNIT_NM', 'ROW_SRC_IND', 'REC_TYP', 'ARRG_MAT_TYP_CD']]\n",
    "    pd.set_option('float_format', '{:f}'.format)\n",
    "\n",
    "    float_data = df.select_dtypes(include=['float','int'])\n",
    "    # Preprocess your dataset if needed\n",
    "    # For example, encoding categorical variables\n",
    "    label_encoders = {}\n",
    "    for column in df.select_dtypes(include=['object']):\n",
    "        encoder = LabelEncoder()\n",
    "        df[column] = encoder.fit_transform(df[column])\n",
    "        label_encoders[column] = encoder\n",
    "\n",
    "    # Perform Min-Max scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    df =df.drop([],axis=1)\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    train_data, test_data = train_test_split(df, test_size=0.3)\n",
    "    # Train and save the model\n",
    "    encoding_dim = 10\n",
    "    # num_epochs = 50\n",
    "    num_epochs = 50\n",
    "    # num_epochs = 500\n",
    "    batch_size = 32\n",
    "    train_and_save_model(train_data, encoding_dim, num_epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8efbf999-457f-453f-964c-091fbdee5167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/50, Loss: 0.03955536708235741\n",
      "Epoch: 20/50, Loss: 0.02710752934217453\n",
      "Epoch: 30/50, Loss: 0.01932833157479763\n",
      "Epoch: 40/50, Loss: 0.01567245088517666\n",
      "Epoch: 50/50, Loss: 0.012111799791455269\n",
      "Threshold for anomaly detection: 0.26949870586395264\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define train_test_split function\n",
    "def train_test_split_mod(df, test_size=0.3):\n",
    "    train_data, test_data = train_test_split(df, test_size=test_size, random_state=seed)\n",
    "    return train_data, test_data\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Train and save the autoencoder model\n",
    "def train_and_save_model(train_data, encoding_dim, num_epochs, batch_size, threshold):\n",
    "    input_dim = train_data.shape[1]\n",
    "    model = Autoencoder(input_dim, encoding_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    train_data = torch.from_numpy(train_data.values).float()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, train_data.size(0), batch_size):\n",
    "            inputs = train_data[i:i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'autoencoder_model_outlier_detection.pth')\n",
    "\n",
    "    # Calculate reconstruction error for all instances\n",
    "    with torch.no_grad():\n",
    "        reconstructions = model(train_data)\n",
    "        mse_loss = nn.MSELoss(reduction='none')\n",
    "        errors = torch.sqrt(mse_loss(reconstructions, train_data)).numpy()\n",
    "\n",
    "    # Determine the threshold for anomaly detection\n",
    "    threshold = np.percentile(errors, threshold)\n",
    "\n",
    "    return threshold\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load your dataset\n",
    "    df = pd.read_csv('AI_sample1.csv')\n",
    "    df = df[['BASE_CCY_AMT', 'FNCT_CCY_AMT', 'TXN_CCY_AMT', 'IB_FLAG', 'TXN_CCY', 'OPER_DPST_FLAG', 'LV_DMC_CTRY', 'FRS_ACCOUNT_CLASS','ACCT_TYP', 'ACTG_UNIT_NM', 'ROW_SRC_IND', 'REC_TYP', 'ARRG_MAT_TYP_CD']]\n",
    "    pd.set_option('float_format', '{:f}'.format)\n",
    "\n",
    "    # Preprocess your dataset if needed\n",
    "    # For example, encoding categorical variables\n",
    "    label_encoders = {}\n",
    "    for column in df.select_dtypes(include=['object']):\n",
    "        encoder = LabelEncoder()\n",
    "        df[column] = encoder.fit_transform(df[column])\n",
    "        label_encoders[column] = encoder\n",
    "\n",
    "    # Perform Min-Max scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    \n",
    "    # Train-test split\n",
    "    train_data, test_data = train_test_split_mod(df_scaled, test_size=0.3)\n",
    "    \n",
    "    # Train the model and get the threshold for anomaly detection\n",
    "    encoding_dim = 10\n",
    "    num_epochs = 50\n",
    "    batch_size = 32\n",
    "    threshold_percentile = 95  # Adjust this percentile based on your dataset\n",
    "    threshold = train_and_save_model(train_data, encoding_dim, num_epochs, batch_size, threshold_percentile)\n",
    "    print(\"Threshold for anomaly detection:\", threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21a5e7db-40cb-4918-bf5a-24b107e9f5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python file 'streamlit_app_1.py' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_python_file(filename):\n",
    "    python_code = \"\"\"\\\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report, f1_score,precision_recall_fscore_support, roc_auc_score,accuracy_score, precision_score, recall_score, f1_score)\n",
    "import webbrowser\n",
    "import os\n",
    "from ydata_profiling import ProfileReport\n",
    "# from pandas_profiling import ProfileReport\n",
    "from streamlit_pandas_profiling import st_profile_report\n",
    "import seaborn as sns\n",
    "import sweetviz as sv\n",
    "from bs4 import BeautifulSoup\n",
    "import subprocess\n",
    "def load_data(file):\n",
    "    return pd.read_csv(file, encoding='utf-8')\n",
    "def float_to_int(x):\n",
    "    return int(x)\n",
    "def pandas_profiling_tab():\n",
    "    st.subheader(\"Pandas Profiling\")\n",
    "    uploaded_file = st.file_uploader(\"Upload CSV file\", type=[\"csv\"])\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        df = load_data(uploaded_file)\n",
    "        # pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "        # df = df[['BASE_CCY_AMT', 'FNCT_CCY_AMT', 'TXN_CCY_AMT', 'IB_FLAG', 'TXN_CCY', 'OPER_DPST_FLAG', 'LV_DMC_CTRY', 'FRS_ACCOUNT_CLASS','ACCT_TYP', 'ACTG_UNIT_NM', 'ROW_SRC_IND', 'REC_TYP', 'ARRG_MAT_TYP_CD']]\n",
    "        # df = df[['BASE_CCY_AMT', 'FNCT_CCY_AMT', 'TXN_CCY_AMT']]\n",
    "        # df = df[['BASE_CCY_AMT','FNCT_CCY_AMT','TXN_CCY_AMT','TXN_CCY','IB_FLAG', 'OPER_DPST_FLAG','LV_DMC_CTRY', 'FRS_ACCOUNT_CLASS','ACCT_TYP', 'ACTG_UNIT_NM', 'ROW_SRC_IND','REC_TYP', 'ARRG_MAT_TYP_CD']]\n",
    "        #df = df[['BASE_CCY_AMT','TXN_CCY_AMT','TXN_CCY','IB_FLAG', 'OPER_DPST_FLAG','LV_DMC_CTRY', 'FRS_ACCOUNT_CLASS','ACCT_TYP', 'ACTG_UNIT_NM', 'ROW_SRC_IND','REC_TYP', 'ARRG_MAT_TYP_CD']]\n",
    "        #df = df.map(lambda x: int(x))\n",
    "        # df = df.applymap(float_to_int)\n",
    "        # float_cols = df.select_dtypes(include=['float64']).columns\n",
    "        # df[float_cols] = df[float_cols].astype('float32').round(2)\n",
    "        profile = ProfileReport(df, explorative=True)\n",
    "        st_profile_report(profile)\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, encoding_dim)\n",
    "        self.decoder = nn.Linear(encoding_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Calculate threshold for anomaly detection\n",
    "def calculate_threshold(errors, percentile):\n",
    "    return np.percentile(errors, percentile)\n",
    "\n",
    "# Perform anomaly detection using the trained autoencoder\n",
    "def detect_anomalies(model, test_data, threshold):\n",
    "    test_data = torch.from_numpy(test_data.values).float()\n",
    "    with torch.no_grad():\n",
    "        reconstructed_data = model(test_data)\n",
    "        mse_loss = torch.mean(torch.pow(test_data - reconstructed_data, 2), dim=1)\n",
    "        anomalies = np.where(mse_loss > threshold)[0]\n",
    "    return anomalies\n",
    "def graph_img(metrics_html, html_table, X, df, shap_value_instances, explainer, feature_df,shap_values,css_content):\n",
    "    \n",
    "    css ='''  .centert {     display: flex;     justify-content: center;     align-items: center;        flex-direction: column; }'''\n",
    "    st.markdown(\n",
    "        f'''\n",
    "        <style>\n",
    "        {css}\n",
    "        </style>\n",
    "        ''',\n",
    "        unsafe_allow_html = True\n",
    "        )\n",
    "    metrics_html = metrics_html.replace('<table', '<table class=\"centert\"')\n",
    "\n",
    "    st.markdown(metrics_html, unsafe_allow_html=True)\n",
    "    \n",
    "    # st.subheader(\"Feature Importance Analysis\")\n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(feature_df)), feature_df['Importance'], tick_label=feature_df['Feature'])\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Importance')\n",
    "    # plt.title('Feature Importance Analysis')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Display feature importances plot\n",
    "    # st.image('feature_importance_plot.png')\n",
    "    # st.subheader(\"SHAP Feature Importance for Global Explanation\")\n",
    "    st.markdown('''\n",
    "    <div style=\"display: flex; justify-content: center;\">\n",
    "        <h3><br><br><br>SHAP Feature Importance for Global Explanation</h3>\n",
    "    </div>''', unsafe_allow_html=True)\n",
    "\n",
    "    # Plot SHAP values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "    # plt.title('SHAP Feature Importance for Global Explanation', fontsize=16)\n",
    "    plt.xlabel('SHAP Value', fontsize=14)\n",
    "    plt.ylabel('Feature', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    labels = ['Normal', 'Anomalous']\n",
    "\n",
    "    # Update legend\n",
    "    plt.legend(handles, labels)\n",
    "    plt.savefig('shap_summary_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Display SHAP values plot\n",
    "    left_co, cent_co,last_co = st.columns(3)\n",
    "    with cent_co:\n",
    "        st.image('shap_summary_plot.png', width=600)\n",
    "    \n",
    "    # st.subheader(\"Anomalous Data\")\n",
    "    st.markdown('''\n",
    "    <div style=\"display: flex; justify-content: center;\">\n",
    "        <h3><br><br><br>Anomalous Data</h3>\n",
    "    </div>''', unsafe_allow_html=True)\n",
    "    \n",
    "    st.markdown(\n",
    "        f'''\n",
    "        <style>\n",
    "        {css_content}\n",
    "        </style>\n",
    "        ''',\n",
    "        unsafe_allow_html = True\n",
    "        )\n",
    "    st.markdown(html_table, unsafe_allow_html=True)\n",
    "def Anomaly_Detection():\n",
    "    st.markdown('''\n",
    "    <div style=\"display: flex; justify-content: center;\">\n",
    "        <h1>Anomaly Detection, using Deep Learning, XAI<br>(Cash & Due Data)</h1>\n",
    "        <h1><br><br></h1>\n",
    "    </div>''', unsafe_allow_html=True)\n",
    "\n",
    "    # st.subheader(\"Anomaly Summary\")\n",
    "    st.markdown('''\n",
    "    <div style=\"display: flex; justify-content: center;\">\n",
    "        <h3><br><br>Anomaly Summary</h3>\n",
    "    </div>''', unsafe_allow_html=True)\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv('AI_sample1.csv')\n",
    "\n",
    "    df=df[['BASE_CCY_AMT', 'FNCT_CCY_AMT', 'TXN_CCY_AMT', 'IB_FLAG', 'TXN_CCY', 'OPER_DPST_FLAG', 'LV_DMC_CTRY', 'FRS_ACCOUNT_CLASS','ACCT_TYP', 'ACTG_UNIT_NM', 'ROW_SRC_IND', 'REC_TYP', 'ARRG_MAT_TYP_CD']]\n",
    "    pd.set_option('float_format', '{:f}'.format)\n",
    "\n",
    "    float_data = df.select_dtypes(include=['float','int'])\n",
    "\n",
    "    ## Convert Categorical variables to factors\n",
    "    # It encodes Categorical variables into numerical factors using LabelEncoder and stores the encoders in a dictionary.\n",
    "    label_encoders = {}\n",
    "    for column in df.select_dtypes(include=['object']):\n",
    "        encoder = LabelEncoder()\n",
    "        df[column] = encoder.fit_transform(df[column])\n",
    "        label_encoders[column] = encoder\n",
    "    scaler = MinMaxScaler()\n",
    "    df =df.drop([],axis=1)\n",
    "    test_df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    model = Autoencoder(input_dim=test_df_scaled.shape[1], encoding_dim=10)\n",
    "    model.load_state_dict(torch.load('autoencoder_model_1.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    # Perform anomaly detection to calculate MSE for test dataset\n",
    "    with torch.no_grad():\n",
    "        reconstructed_data = model(torch.from_numpy(test_df_scaled.values).float())#d2\n",
    "        mse_loss = torch.mean(torch.pow(torch.from_numpy(test_df_scaled.values).float() - reconstructed_data, 2), dim=1) #d1\n",
    "\n",
    "    # Calculate threshold for anomaly detection\n",
    "    threshold = calculate_threshold(mse_loss.numpy(), percentile=95)\n",
    "\n",
    "    # Perform anomaly detection using the calculated threshold\n",
    "    anomalies = detect_anomalies(model, test_df_scaled, threshold)\n",
    "    new_df = df.copy()\n",
    "    new_df['Anomaly_Found'] = new_df.index.isin(anomalies).astype(int)\n",
    "    new_df.to_csv('pp.csv', index=False) \n",
    "    # per_df= test_data.copy()\n",
    "    # per_df['Anomaly_Found'] =0\n",
    "    # per_df.iloc[anomalies,-1]=1\n",
    "    X = new_df.drop(columns=['Anomaly_Found'])\n",
    "    y = new_df['Anomaly_Found']\n",
    "    clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = clf.feature_importances_\n",
    "    feature_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "    feature_df = feature_df.sort_values(by='Importance', ascending=False)\n",
    "    # Use SHAP to explain the model's predictions\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    anomalies = new_df[new_df['Anomaly_Found'] == 1]\n",
    "    anomalies_rows=len(anomalies)\n",
    "    test_dataset_rows = len(df)\n",
    "    per=round(((anomalies_rows/test_dataset_rows)*100),2)\n",
    "    error_df = pd.DataFrame({'reconstruction_error': mse_loss, 'true_class':y})\n",
    "    css_content = '''\n",
    "    table {\n",
    "        border-collapse: collapse;\n",
    "        margin: 20px auto;\n",
    "        overflow-y:scroll;\n",
    "        display:block;\n",
    "        border: none;\n",
    "    }\n",
    "    th, td {\n",
    "        padding: 8px;\n",
    "        text-align: center;\n",
    "        border-bottom: 1px solid #ddd;\n",
    "    }\n",
    "    th{\n",
    "        background-color: #007bff;\n",
    "        color: #fff; /* White text color for heading */\n",
    "    }\n",
    "    .blue-row {\n",
    "        background-color: #cceeff; /* Blue color */\n",
    "    }\n",
    "    .white-row {\n",
    "        background-color: #ffffff; /* White color */\n",
    "    }\n",
    "    '''\n",
    "    css = '''  .center {     display: flex;     justify-content: center;     align-items: center;     height: 100vh;     flex-direction: column; }'''\n",
    "    css_content = css_content + css\n",
    "    html_table = anomalies.to_html(index=True, header=True, classes='my-table')\n",
    "    html_table = html_table.replace('<table', '<table style=\"width: 1200px; height: 600px; border-collapse: collapse;\"')\n",
    "    html_table = html_table.replace('<tbody>', '<tbody>')\n",
    "\n",
    "    # Alternating row colors\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        if i % 2 == 0:\n",
    "            html_table = html_table.replace(f'<tr>', f'<tr class=\"blue-row\">', 1)\n",
    "        else:\n",
    "            html_table = html_table.replace(f'<tr>', f'<tr class=\"white-row\">', 1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    threshold_copy = str(round(threshold, 2))\n",
    "    with open(\"style.css\",\"w\") as css_file:\n",
    "        css_file.write(css_content)\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "    shap_value_instances = explainer.shap_values(X) \n",
    "    # Assuming you have true labels for anomalies in your dataset\n",
    "    true_labels = new_df['Anomaly_Found']\n",
    " \n",
    "    # Assuming you have predictions from your model\n",
    "    # You might need to adjust this depending on how you're obtaining predictions\n",
    "    # predicted_labels = clf.predict(X)\n",
    "    df2 = pd.read_csv(\"standard.csv\")\n",
    "    predicted_labels= df2['Outliers']\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    " \n",
    "    # Calculate precision\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    " \n",
    "    # Calculate recall\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    " \n",
    "    # Calculate F1-score\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    metrics_data = {\n",
    "        # 'Metric': ['No of rows in the record:', 'No of anomalies rows in the record:', 'What percentage of data is anomalous:', 'Mean Square Error Threshold'],\n",
    "        'Metric': ['No of rows in the record:', 'No of anomalies rows in the record:', 'What percentage of data is anomalous:','Accuracy','Precision','Recall','F1-score'],\n",
    "        # 'Metric': ['No of rows in the record:', 'No of anomalies rows in the record:', 'What percentage of data is anomalous:'],\n",
    "        # 'Value': [test_dataset_rows, anomalies_rows, f'{per}%', threshold_copy]\n",
    "        'Value': [test_dataset_rows, anomalies_rows, f'{per}%',accuracy,precision,recall,f1]\n",
    "        # 'Value': [test_dataset_rows, anomalies_rows, f'{per}%']\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_html = metrics_df.to_html(index=False, header=False, classes='my-table')\n",
    "    metrics_html = metrics_html.replace('<table', '<table class=\"table table-striped\" id=\"my-table\"')\n",
    "    graph_img(metrics_html, html_table, X, df, shap_value_instances, explainer, feature_df,shap_values,css_content)\n",
    "    two_d_list = [item for sublist in shap_value_instances for item in sublist]\n",
    "    shap_value = pd.DataFrame(two_d_list)\n",
    "    shap_abs = abs(shap_value)\n",
    "    a = new_df.drop(columns=['Anomaly_Found'])\n",
    "    result_df = pd.DataFrame(0, index=range(len(new_df)), columns=a.columns)\n",
    "    for i in range(len(new_df)):\n",
    "        shap_row = shap_abs.iloc[i]\n",
    "        top3_features_indices = shap_row.argsort()[:-6:-1]  # Get indices of top 3 features\n",
    "        result_df.loc[i, a.columns[top3_features_indices]] = 1\n",
    "    result_df['index']=new_df.index\n",
    "    result_df = result_df.iloc[anomalies.index]\n",
    "    result_df.to_csv('anomaly_features.csv', index=False)    \n",
    "    y_pred =predicted_labels\n",
    "\n",
    "    # # Create a confusion matrix\n",
    "    # cm = confusion_matrix(true_labels, y_pred)\n",
    "\n",
    "    # # Plot the confusion matrix\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    # plt.xlabel(\"Predicted labels\")\n",
    "    # plt.ylabel(\"True labels\")\n",
    "    # plt.title(\"Confusion Matrix\")\n",
    "    # plt.savefig('Confusion_matrix.png')\n",
    "    # plt.clf()\n",
    "    # st.image('Confusion_matrix.png')\n",
    "    LABELS = ['Normal', 'Fraud']\n",
    "    # y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
    "    conf_matrix = confusion_matrix(true_labels, y_pred)\n",
    "    sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", cmap='Blues');\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.savefig('Confusion_matrix.png')\n",
    "    plt.clf()\n",
    "    st.image('Confusion_matrix.png')\n",
    "    \n",
    "    st.title('Search Row Number')\n",
    "    instance_index = st.number_input('Enter Row Number:', min_value=0, max_value=len(df)-1, value=0, step=1)\n",
    "    def plot_local_shap_explanation(instance_index):\n",
    "        instances = X.iloc[[instance_index]]\n",
    "        # Plot the waterfall plot\n",
    "        shap.waterfall_plot(shap.Explanation(values=shap_value_instances[1][instance_index], base_values=explainer.expected_value[1], data=instances.iloc[0]), max_display=10)\n",
    "        plt.title('SHAP Waterfall Plot')  # Add a title\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_local_explanation.png')\n",
    "        plt.clf()\n",
    "    if st.button('Search'):\n",
    "        plot_local_shap_explanation(instance_index)\n",
    "        st.title(f'SHAP Local Explanation for Row {instance_index}')\n",
    "        left_co, cent_co,last_co = st.columns(3)\n",
    "        with cent_co:\n",
    "            st.image('shap_local_explanation.png',width = 700)\n",
    "if __name__ == '__main__':\n",
    "    command = ['pip', 'install', 'shap==0.42.1']\n",
    "    try:\n",
    "        subprocess.check_call(command)\n",
    "        st.set_page_config(layout='wide')\n",
    "        # Load your preprocessed test dataset\n",
    "        #tabs =[\"Anomaly Detection\",\"Pandas Profiling Report\"]\n",
    "        st.title(\"Data Quality Platform\")\n",
    "        tab1, tab2 = st.tabs([\"Pandas Profiling Report\",\"Anomaly Detection\"])#\"Sweetviz Report\"])\n",
    "        # User Input\n",
    "        with tab1:\n",
    "            pandas_profiling_tab()\n",
    "        with tab2:\n",
    "            Anomaly_Detection()\n",
    "        \n",
    "        # with tab3:\n",
    "        #     df = load_data('pp.csv')\n",
    "        #     report = sv.analyze(df.\"Anomaly_Found\")\n",
    "        #     # Display Sweetviz report in Streamlit\n",
    "        #     st.write(report.show_html(), unsafe_allow_html=True)\n",
    "    except subprocess.CalledProcessError:\n",
    "        st.title(\"404 not found!\")\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(python_code)\n",
    "    print(f\"Python file '{filename}' has been created successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = \"streamlit_app_1.py\"\n",
    "    create_python_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7c6ea2b-2333-4487-aea8-a06272a3d735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run streamlit_app_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10eab6e4-7d01-4de5-ad3a-80ecf340881c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while generating the Pandas Profiling report.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "# Specify the name of the CSV file \n",
    "csv_file_name = \"pp.csv\" \n",
    "# Define the command to run Pandas Profiling in CLI mode \n",
    "command = [ \"python\", \"-m\", \"pandas_profiling\", csv_file_name, \"--output\", \"output_report.html\" ] \n",
    "# Execute the command \n",
    "completed_process = subprocess.run(command, capture_output=True) \n",
    "# Check the return code to ensure the command executed successfully \n",
    "if completed_process.returncode == 0:\n",
    "    print(\"Pandas Profiling report generated successfully.\") \n",
    "else:\n",
    "    print(\"An error occurred while generating the Pandas Profiling report.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e570d-bf89-4212-b026-a47be293b3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
